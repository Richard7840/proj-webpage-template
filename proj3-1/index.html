<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background-color: #121212;
    padding: 100px;
    width: 1000px;
	  font-size: 20px;
	  line-height: 1.5;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: white;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  kbd {
    color: salmon;
  }
  th, td {
  padding: 5px;
  }
</style>
<title>CS 184 Path Tracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
<h1 align="middle">Project 3-1: Path Tracer</h1>
<h2 align="middle">Richard Lew, James Hu</h2>

<!-- Add Website URL -->
<!-- <h2 align="middle">Website URL: <a href="TODO">TODO</a></h2> ?????-->

<br><br>


<div align="center">
  <table style="width:100%">
      <tr>
          <td align="middle">
          <img src="Task5_bunny.png" width="480px" />
          <!-- <figcaption align="middle">Results Caption: my bunny is the tastiest bunny</figcaption> -->
      </tr>
  </table>
</div>

<!-- <p>All of the text in your write-up should be <em>in your own words</em>. If you need to add additional HTML features to this document, you can search the <a href="http://www.w3schools.com/">http://www.w3schools.com/</a> website for instructions. To edit the HTML, you can just copy and paste existing chunks and fill in the text and image file names appropriately.</p>
<o>The website writeup is intended to be a self-contained walkthrough of the assignment: we want this to be a piece of work which showcases your understanding of relevant concepts through both mesh images as well as written explanations about what you did to complete each part of the assignment. Try to be as clear and organized as possible when writing about your own output files or extensions to the assignment. We want to understand what you've achieved and how you've done it!</p> 
<p>If you are well-versed in web development, feel free to ditch this template and make a better looking page.</p>


<p>Here are a few problems students have encountered in the past. Test your website on the instructional machines early!</p>
<ul>
<li>Your main report page should be called index.html.</li>
<li>Be sure to include and turn in all of the other files (such as images) that are linked in your report!</li>
<li>Use only <em>relative</em> paths to files, such as <pre>"./images/image.jpg"</pre>
Do <em>NOT</em> use absolute paths, such as <pre>"/Users/student/Desktop/image.jpg"</pre></li>
<li>Pay close attention to your filename extensions. Remember that on UNIX systems (such as the instructional machines), capitalization matters. <pre>.png != .jpeg != .jpg != .JPG</pre></li>
<li>Be sure to adjust the permissions on your files so that they are world readable. For more information on this please see this tutorial: <a href="http://www.grymoire.com/Unix/Permissions.html">http://www.grymoire.com/Unix/Permissions.html</a></li>
<li>And again, test your website on the instructional machines early!</li> -->
</ul>


<!-- <p>Here is an example of how to include a simple formula:</p>
<p align="middle"><pre align="middle">a^2 + b^2 = c^2</pre></p>
<p>or, alternatively, you can include an SVG image of a LaTex formula.</p> -->

<div>

<h2 align="middle">Overview</h2>
<p>
  This project focuses on estimating the lighting of a geometry from a perspective by tracing rays from our "camera" into the scene. The first step was to generate these rays which can be defined by their origin and their direction. In order to detect what the ray collides with, we need to determine whether the ray collides with the various objects in our scene. We implement triangle intersection by defining the intersection of the ray and the plane of the triangle as barycentric coordinates, and if the coordinates are valid we know it intersected the triangle. Sphere intersection is implemented by solving a quadratic equation derived from the ray equation and the equation for a point on a sphere.
  <br>
  In order to efficiently detect what objects a ray intersects with, we create a binary tree structure called the Bounding Volume Hierarchy which stores bounding boxes that hold groups of objects, and every level stores bounding boxes for roughly half of the objects of the previous level. This makes a previous linear computation over all n objects a log n computation due to the binary tree structure.
  <br>
  To find what lighting comes from a given ray, we use monte carlo estimation to weight the radiance along a ray, and shoot multiple rays randomly into the scene through every pixel. We take the first object it intersects with and find the illuminance that comes from that object at that point. We can be efficient with the number of rays we shoot per pixel by detecting when the illuminance in the pixel has converged with high confidence so we can reduce unnecessary sampling.
  <br>
  For direct illumination, we only care about the light either emitted directly from the geometry, or light reflected from a light source. We can efficiently find the latter by drawing rays from the point we initially intersected with the scene towards the lights in the scene and to find if we are in a shadow or not. By weighting the light coming from every light source we can monte carlo estimate the light reflected from the point in the scene.
  <br>
  For global lighting we now take into account multiple light reflections, and so we now calculate the light coming from multiple bounces in the scene. Instead of just looking at what light comes to the first intersection, we now shoot another random ray into the scene, and see how much light reflects from that point, etc. In theory we'd do this an infinite number of times to calculate the actual illuminance, but in practice we make this process finite by using a russian roulette short-circuit to cut the infinite bouncing by a constant probability every iteration.
  
</p>
<br>

<h2 align="middle">Part 1: Ray Generation and Scene Intersection (20 Points)</h2>
<!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
Explain the triangle intersection algorithm you implemented in your own words.
Show images with normal shading for a few small .dae files. -->

<h3>
  Walk through the ray generation and primitive intersection parts of the rendering pipeline.
</h3>
<p>
  In order to generate rays in our world, we need to perform transformations from image to camera space, and camera to world space. These are simple affine transformations and can be described with linear transformations.
  <br>
  We estimate the integral of radiance incident upon a pixel by shooting multiple random rays out of every pixel into the world as samples. These rays sample the world contained in its respective pixel and the pixel displays the average of the samples.
  <br>
  In order to sample the world, primitive intersection allows the renderer to "see" various geometries by recording when and how the ray interacts with triangles and spheres. We want to know if a ray intersects with a given primitive, and, if it does, we can know where it intersects, the normal at the intersection, and the bsdf of the material it intersects with.
  
</p>
<br>

<h3>
  Explain the triangle intersection algorithm you implemented in your own words.
</h3>
<p>
  Our triangle intersection algorithm implements the Moller Trumbore Algorithm. In essence, we solve the linear equation of the intersection between the ray and the plane, where the plane is expressed as barycentric coordinates of the triangle. If it intersects with the triangle, the barycentric coordinates will be "valid": be positive and sum to 1. The solution includes the value of the parameter t at which the ray intersects the triangle, and if that value of t is within the valid range of t for this ray, then we record a collision/intersection for the renderer.
</p>
<br>

<h3>
  Show images with normal shading for a few small .dae files.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="Task1_banana.png" align="middle" width="480px"/>
        <figcaption>banana.dae</figcaption>
      </td>
      <td>
        <img src="Task1_gems_s4.png" align="middle" width="480px"/>
        <figcaption>gems.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="Task1_lambert.png" align="middle" width="480px"/>
        <figcaption>spheres_lambertian.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


<h2 align="middle">Part 2: Bounding Volume Hierarchy (20 Points)</h2>
<!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

<h3>
  Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
</h3>
<p>
  Our BVH construction algorithm followed the basic binary tree algorithm, where we loop through the primitives three times in order to sort and derive the necessary info to create the split. First we loop through it to find the centroid, and if there are too few primitives, we make the node a leaf. Then we loop through it again to find which axis to split along the centroid, then we loop through it a final time to construct the primitive lists for the children recursive calls. The left and right children of an interior node are recursive calls with the split primitive lists as inputs.
</p>

<h3>
  Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="Task2_beast.png" align="middle" width="480px"/>
        <figcaption>beast.dae</figcaption>
      </td>
      <td>
        <img src="Task2_dragon.png" align="middle" width="480px"/>
        <figcaption>dragon.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="Task2_walle.png" align="middle" width="480px"/>
        <figcaption>wall-e.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.
</h3>
<p>
  <table border="1px white" align="middle">
    <thead>
        <tr>
            <th colspan="4">Comparison of Rendering Times with BVH</th>
        </tr>
    </thead>
    <tbody>
        <tr>
          <td>Mesh dae</td>
          <td>Primitive count</td>
          <td>Render Time w/o BVH</td>
          <td>Render Time w/ BVH</td>
        </tr>
        <tr>
          <td>meshedit/cow.dae</td>
          <td>5856</td>
          <td>50.53 sec</td>
          <td>0.07 sec</td>
        </tr>
        <tr>
          <td>sky/bunny.dae</td>
          <td>33696</td>
          <td>297.51 sec</td>
          <td>0.14 sec</td>
        </tr>
        <tr>
          <td>meshedit/maxplanck.dae</td>
          <td>50801</td>
          <td>452.70 sec</td>
          <td>0.29 sec</td>
        </tr>
    </tbody>
</table>
<br>
<div align="center">
  <table style="width:100%">
    <tr>
      <td align="middle">
      <img src="Task2_speed_chart.png" align="middle" width="400px"/>
      <figcaption># primitives vs time to render</figcaption>
    </tr>
  </table>
</div>
<br>
The BVH acceleration gave a ~1000x speed up, and had a better slow down curve as the primitives increased. The traversal of the BVH tree is O(log N) time, where N is the number of primitives, while the traversal of all the primitives w/o the speed up is O(N) time. The O(log N) time is shown in the normalized plot below which shows how long it takes for each method to render a certain number of primitives. They are normalized by their speed for the smallest data point, otherwise BVH acceleration wouldn't show up meaningfully. As can be seen, no acceleration is very linear in N, while BVH acceleration is strictly sublinear, not taking into account the constant ~1000 speed up.
</p>
<br>

<h2 align="middle">Part 3: Direct Illumination (20 Points)</h2>
<!-- Walk through both implementations of the direct lighting function.
Show some images rendered with both implementations of the direct lighting function.
Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

<h3>
  Walk through both implementations of the direct lighting function.
</h3>
<p>
  The direct lighting functions are used to estimate the lighting at a point in the geometry via pathtracing. The first implementation with uniform hemisphere sampling works by sending a constant number of random rays picked uniformly from a hemisphere from the point and monte carlo estimating the lighting by averaging the light coming from each of the random rays.
  The optimized direct lighting function uses importance sampling such that rays are sampled from light sources in the geometry, instead of just randomly in an hemisphere. Every light is randomly sampled a certain number of times except for point lights which only need to be sampled once, and their illuminance is averaged through monte carlo estimation.
</p>

<h3>
  Show some images rendered with both implementations of the direct lighting function.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <!-- Header -->
    <tr align="center">
      <th>
        <b>Uniform Hemisphere Sampling</b>
      </th>
      <th>
        <b>Light Sampling</b>
      </th>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="Task3_bunny_Hs64l32.png" align="middle" width="480px"/>
        <figcaption>bunny.dae</figcaption>
      </td>
      <td>
        <img src="Task3_bunny_Is64l32.png" align="middle" width="480px"/>
        <figcaption>bunny.dae</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="Task3_spheres_Hs64l32.png" align="middle" width="480px"/>
        <figcaption>spheres_lambertian.dae</figcaption>
      </td>
      <td>
        <img src="Task3_spheres_Is64l32.png" align="middle" width="480px"/>
        <figcaption>spheres_lambertian.dae</figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>
<br>

<h3>
  Focus on one particular scene with at least one area light and compare the noise levels in <b>soft shadows</b> when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, <b>not</b> uniform hemisphere sampling.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="Task3_spheres_shadow_study_s1l1.png" align="middle" width="480px"/>
        <figcaption>1 Light Ray (spheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="Task3_spheres_shadow_study_s1l4.png" align="middle" width="480px"/>
        <figcaption>4 Light Rays (spheres_lambertian.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="Task3_spheres_shadow_study_s1l16.png" align="middle" width="480px"/>
        <figcaption>16 Light Rays (spheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="Task3_spheres_shadow_study_s1l64.png" align="middle" width="480px"/>
        <figcaption>64 Light Rays (spheres_lambertian.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
  As we increase the lighting samples the shadows in the scene get a softer gradient. With 1 light ray, there are no light rays to average over, so the shadow either exists or it doesn't, leading to pure black shadows that are probabilistically either black or the color of the material underneath. This can be seen best in the top left png. As we increase the number of the light rays this average begins to converge for every pixel and the shadows begin to have values that can vary. A single point in the soft part of the shadow may be able to see only part of the area light, and the samples will be able to calculate this average and make the shadow softer.
</p>
<br>

<h3>
  Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.
</h3>
<p>
  The difference between uniform hemisphere and lighting sampling in our renders is clear in the images above. Lighting sampling has much smoother colors on surfaces with much fewer samples. Shadows are also better defined and less noisy. The hemisphere sampling images seem grainy because some pixels will not find any lights with their light rays, making them much darker than their neighbors which may probabilistically find the lights. Importance sampling ensures we only look at the rays that matter to us, and by weighting the samples correctly we can get better images with much less work.
</p>
<br>


<h2 align="middle">Part 4: Global Illumination (20 Points)</h2>
<!-- Walk through your implementation of the indirect lighting function.
Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

<h3>
  Walk through your implementation of the indirect lighting function.
</h3>
<p>
  Indirect lighting was implemented recursively by calculating the light from one bounce, then adding the light reflected from that point from another bounce. This recursion is repeated a maximum of max_ray_depth times, but can be short circuited if a ray misses the geometry (no light to reflect), or by a russian roulette probability function which will cut off the recursion with constant probability every iteration. Each bounce is randomly picked from the hemisphere around the normal of its incident collision. The lighting is recursively calculated with monte carlo estimation.
  In summary, the method is multiple one bounce calculations implemented with successive ray tracing from each ray intersection to the next.
  
</p>
<br>

<h3>
  Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="task4_CBspheres_l4m5.png" align="middle" width="480px"/>
        <figcaption>CBspheres.dae</figcaption>
      </td>
      <td>
        <img src="task4_dragon_l4m5.png" align="middle" width="480px"/>
        <figcaption>dragon.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="task4_walle_l4m5.png" align="middle" width="480px"/>
        <figcaption>wall-e.dae</figcaption>
      </td>
      <td>
        <img src="task4_bunny_m100.png" align="middle" width="480px"/>
        <figcaption>bunny.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="task4_bunny_m1.png" align="middle" width="480px"/>
        <figcaption>Only direct illumination (bunny.dae)</figcaption>
      </td>
      <td>
        <img src="task4_bunny_indirect_s1024l4.png" align="middle" width="480px"/>
        <figcaption>Only indirect illumination (bunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
  The scene with only direct illumination has pure black shadows since only up to one bounce is being calculated, and there is no color bleeding. The underside of the bunny is black because there is no direct path to the area light. The indirect illumination scene shows that a decent amount of illumination is from 2+ bounces. Here the area light is black because zero bounce light isn't included, and the overall scene is darker, but color bleeding is much more apparent, and the exposed undersides of the bunny are lit from light bouncing off the walls and floor.
</p>
<br>

<h3>
  For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="task4_bunny_m0.png" align="middle" width="480px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="task4_bunny_m1.png" align="middle" width="480px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="task4_bunny_m2.png" align="middle" width="480px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="task4_bunny_m3.png" align="middle" width="480px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="task4_bunny_m100.png" align="middle" width="480px"/>
        <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    The first two scenes with max_ray_depth = 0, 1 correspond to direct lighting, and therefore are the most important visually. With max_ray_depth > 1 we always have at least one extra bounce, and so most of the black shadows become softer. The largest difference for global illumination is from max_ray_depth 1 to 2, which is where we begin to color bleeding and the undersides of the bunny begin to be illuminated. Increasing the max_ray_depth from there gives diminishing returns. From 2 to 3 we see the shadows brighten a little, and from 3 to 100 there is a near imperceptible brightening as well. Of course, the computation time also statistically doesn't increase linearly with max_ray_depth due to the russian roulette sampling, so increasing the depth has diminishing returns here as well.
</p>
<br>

<h3>
  Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="task4_CBspheres_s1l4m5.png" align="middle" width="480px"/>
        <figcaption>1 sample per pixel (CBspheres.dae)</figcaption>
      </td>
      <td>
        <img src="task4_CBspheres_s2l4m5.png" align="middle" width="480px"/>
        <figcaption>2 samples per pixel (CBspheres.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="task4_CBspheres_s4l4m5.png" align="middle" width="480px"/>
        <figcaption>4 samples per pixel (CBspheres.dae)</figcaption>
      </td>
      <td>
        <img src="task4_CBspheres_s8l4m5.png" align="middle" width="480px"/>
        <figcaption>8 samples per pixel (CBspheres.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="task4_CBspheres_s16l4m5.png" align="middle" width="480px"/>
        <figcaption>16 samples per pixel (CBspheres.dae)</figcaption>
      </td>
      <td>
        <img src="task4_CBspheres_s64l4m5.png" align="middle" width="480px"/>
        <figcaption>64 samples per pixel (CBspheres.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="task4_CBspheres_l4m5.png" align="middle" width="480px"/>
        <figcaption>1024 samples per pixel (CBspheres.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
  The renders with low pixel sampling rates are much grainier, and the colors/shadows are much rougher. For example, with a pixel sampling rate of 1, the color at every pixel is whatever illuminance comes from that single ray. Therefore, adjacent pixels can be completely different colors, even if we expected the colors to be smooth. One pixel from a shadow may have had its ray fly out of the scene and therefore be black, while an adjacent pixel may have bounced off a red wall first. This would have a black pixel next to a light red pixel, which can be seen in the first four renders. As the samples increase, the monte carlo estimation of the illuminance integrals begins to converge and the colors become smoother and more accurate.
</p>
<br>


<h2 align="middle">Part 5: Adaptive Sampling (20 Points)</h2>
<!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

<h3>
  Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
</h3>
<p>
  Adaptive Sampling is a method used to identify when a pixel converges from sampling, and short-circuits the sampling process to save on computational power for pixels that don't need to utilize the max number of samples. To detect convergence we calculate a confidence interval using the values of our samples, and periodically check if the average illuminance from our samples lies within this interval with 95% confidence.
  <br>
  In our implementation we calculate the deviation from the mean with running averages of the sample illuminance, and the illuminance squared, and use these values to calculate the standard deviation and mean. From this we can determine whether the mean has settled within the 95% confidence interval, and if it has, we stop gathering samples.
  
</p>
<br>

<h3>
  Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="Task5_bunny.png" align="middle" width="480px"/>
        <figcaption>Rendered image (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="Task5_bunny_rate.png" align="middle" width="480px"/>
        <figcaption>Sample rate image (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="Task5_banana.png" align="middle" width="480px"/>
        <figcaption>Rendered image (banana.dae)</figcaption>
      </td>
      <td>
        <img src="Task5_banana_rate.png" align="middle" width="480px"/>
        <figcaption>Sample rate image (banana.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<h2 align="middle">Partner Collaboration</h2>
<p>
  We collaborated mostly in person with pair programming. We'd both bring our laptops to a library and code in tandem on the same parts. We both had computers with good processors so having two machines made churning out some of the later renders easier. When time was a bit tight, we'd sometimes do some of the debugging or tedious work at home solo, then present our work the next time we were in person before getting back up to speed. The main problem we ran into was the practical one of finding good times and places to meet, and planning around library hours. Luckily our schedules worked out well enough and we were able to meet semi-regularly.
  <br>
  The project taught us how to write estimations of concrete mathematical functions in practical time-efficient code. We learned a little about time management, and a lot about debugging C++. In terms of collaboration, we learned how to explain ideas and bugs to a partner, even if we didn't fully understand it at the time, and how to work them out together. 
</p>
</body>
</html>
